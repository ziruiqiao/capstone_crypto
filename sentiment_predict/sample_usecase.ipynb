{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from sentiment_predict.predict_sentiment_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8790043cba354ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = get_targets_posts(\"ETH\")\n",
    "result = get_sentiment_score_seq(posts)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b693ad0fd5b5a73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 10200\n",
      "Validation samples: 1275\n",
      "Test samples: 1275\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from Hugging Face\n",
    "dataset = load_dataset(\"arad1367/Crypto_Fundamental_News\")\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "label_mapping = {\"positive\": 2, \"neutral\": 1, \"negative\": 0}\n",
    "df[\"label\"] = df[\"label\"].map(label_mapping)\n",
    "\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    df[\"text\"].tolist(), df[\"label\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a01c34543fed873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikiq\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=3,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fed2560a6996efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "val_dataset = CustomDataset(val_encodings, val_labels)\n",
    "test_dataset = CustomDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a3f5724ddb59aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikiq\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "comet_ml version 3.43.0 is installed, but version 3.43.2 or higher is required. Please update comet_ml to the latest version to enable Comet logging with pip install 'comet-ml>=3.43.2'.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c38a228d4dd4eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6375' max='6375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6375/6375 14:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.601100</td>\n",
       "      <td>0.534296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.440800</td>\n",
       "      <td>0.489859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.277900</td>\n",
       "      <td>0.432045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.288500</td>\n",
       "      <td>0.418756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>0.364132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6375, training_loss=0.48120492317162306, metrics={'train_runtime': 875.3006, 'train_samples_per_second': 58.266, 'train_steps_per_second': 7.283, 'total_flos': 2751948476550000.0, 'train_loss': 0.48120492317162306, 'epoch': 5.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee466a383a7a191f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3646767735481262,\n",
       " 'eval_runtime': 5.8423,\n",
       " 'eval_samples_per_second': 218.237,\n",
       " 'eval_steps_per_second': 27.387,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "43223a856e80bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    predicted_index = torch.argmax(probabilities).item()\n",
    "    confidence = probabilities[0, predicted_index].item()\n",
    "    predicted_label = list(label_mapping.keys())[list(label_mapping.values()).index(predicted_index)]\n",
    "\n",
    "    return predicted_label, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b2cf2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentiment_predict.predict_sentiment_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa49f18945246342",
   "metadata": {},
   "outputs": [],
   "source": [
    "post = get_targets_posts(\"btc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f76a87b4782b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for item in post:\n",
    "    content = item.__dict__['selftext']\n",
    "    title = item.__dict__['title']\n",
    "    texts.append((title, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f43329527b98626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative', 0.9574872255325317)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predict(\" \".join(texts[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85a9f7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BCH Argentina reached 50% of its Flipstarter campaign! Help us complete it and promote the use of BCH in Argentina',\n",
       " '')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c42e84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./deberta_sentiment\\\\tokenizer_config.json',\n",
       " './deberta_sentiment\\\\special_tokens_map.json',\n",
       " './deberta_sentiment\\\\spm.model',\n",
       " './deberta_sentiment\\\\added_tokens.json',\n",
       " './deberta_sentiment\\\\tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./deberta_sentiment\")\n",
    "tokenizer.save_pretrained(\"./deberta_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f353526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model from the saved directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./deberta_sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./deberta_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ddc8984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative', 0.9574872255325317)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\" \".join(texts[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59464033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
